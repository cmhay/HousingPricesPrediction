{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Read CSV file\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
    "\n",
    "# Feature engineering\n",
    "import math\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (1121, 37)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>...</td>\n",
       "      <td>548.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>...</td>\n",
       "      <td>460.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>...</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>642.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2006.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>...</td>\n",
       "      <td>836.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>1456.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>1457.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>...</td>\n",
       "      <td>500.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>1458.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>...</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>1459.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>1460.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>...</td>\n",
       "      <td>276.0</td>\n",
       "      <td>736.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2        3    4    5       6       7      8      9   \\\n",
       "0        1.0  60.0  65.0   8450.0  7.0  5.0  2003.0  2003.0  196.0  706.0   \n",
       "1        2.0  20.0  80.0   9600.0  6.0  8.0  1976.0  1976.0    0.0  978.0   \n",
       "2        3.0  60.0  68.0  11250.0  7.0  5.0  2001.0  2002.0  162.0  486.0   \n",
       "3        4.0  70.0  60.0   9550.0  7.0  5.0  1915.0  1970.0    0.0  216.0   \n",
       "4        5.0  60.0  84.0  14260.0  8.0  5.0  2000.0  2000.0  350.0  655.0   \n",
       "...      ...   ...   ...      ...  ...  ...     ...     ...    ...    ...   \n",
       "1116  1456.0  60.0  62.0   7917.0  6.0  5.0  1999.0  2000.0    0.0    0.0   \n",
       "1117  1457.0  20.0  85.0  13175.0  6.0  6.0  1978.0  1988.0  119.0  790.0   \n",
       "1118  1458.0  70.0  66.0   9042.0  7.0  9.0  1941.0  2006.0    0.0  275.0   \n",
       "1119  1459.0  20.0  68.0   9717.0  5.0  6.0  1950.0  1996.0    0.0   49.0   \n",
       "1120  1460.0  20.0  75.0   9937.0  5.0  6.0  1965.0  1965.0    0.0  830.0   \n",
       "\n",
       "      ...     27     28    29     30   31   32   33      34    35      36  \n",
       "0     ...  548.0    0.0  61.0    0.0  0.0  0.0  0.0     0.0   2.0  2008.0  \n",
       "1     ...  460.0  298.0   0.0    0.0  0.0  0.0  0.0     0.0   5.0  2007.0  \n",
       "2     ...  608.0    0.0  42.0    0.0  0.0  0.0  0.0     0.0   9.0  2008.0  \n",
       "3     ...  642.0    0.0  35.0  272.0  0.0  0.0  0.0     0.0   2.0  2006.0  \n",
       "4     ...  836.0  192.0  84.0    0.0  0.0  0.0  0.0     0.0  12.0  2008.0  \n",
       "...   ...    ...    ...   ...    ...  ...  ...  ...     ...   ...     ...  \n",
       "1116  ...  460.0    0.0  40.0    0.0  0.0  0.0  0.0     0.0   8.0  2007.0  \n",
       "1117  ...  500.0  349.0   0.0    0.0  0.0  0.0  0.0     0.0   2.0  2010.0  \n",
       "1118  ...  252.0    0.0  60.0    0.0  0.0  0.0  0.0  2500.0   5.0  2010.0  \n",
       "1119  ...  240.0  366.0   0.0  112.0  0.0  0.0  0.0     0.0   4.0  2010.0  \n",
       "1120  ...  276.0  736.0  68.0    0.0  0.0  0.0  0.0     0.0   6.0  2008.0  \n",
       "\n",
       "[1121 rows x 37 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import training CSV\n",
    "fname = './data/train.csv'\n",
    "# use pandas to read the CSV properly of all different types\n",
    "data_pd = pd.read_csv(fname)\n",
    "# convert to a numpy array\n",
    "data = np.array(data_pd)\n",
    "# remove categorical data - force cast attempt then remove failures\n",
    "cols_to_remove = []\n",
    "for i in range(data.shape[1]):\n",
    "    try:\n",
    "        data[:,i] = data[:,i].astype(float)\n",
    "    except:\n",
    "        cols_to_remove.append(i)\n",
    "\n",
    "data = np.delete(data, cols_to_remove, axis=1)\n",
    "# remove rows with nan values\n",
    "rows_to_remove = []\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        if math.isnan(data[i,j]):\n",
    "            rows_to_remove.append(i)\n",
    "            break\n",
    "\n",
    "data = np.delete(data, rows_to_remove, axis=0)\n",
    "\n",
    "# remove outliers with standard deviation TODO()\n",
    "\n",
    "# take the actual prices and add them to results y\n",
    "y = data[:, data.shape[1] - 1]\n",
    "# remove prices column from data\n",
    "data = np.delete(data, data.shape[1] - 1, 1)\n",
    "data = data.astype(float)\n",
    "print(\"data shape\", data.shape)\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CHECKPOINT 5][5 points]\n",
    "def  feature_normalization(X):\n",
    "    \"\"\"\n",
    "    Normalizes the features in X. returns a normalized version of X where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_norm : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    First, for each feature dimension, compute the mean of the feature\n",
    "    and subtract it from the dataset, storing the mean value in mu. \n",
    "    Next, compute the  standard deviation of each feature and divide\n",
    "    each feature by it's standard deviation, storing the standard deviation \n",
    "    in sigma. \n",
    "    \n",
    "    Note that X is a matrix where each column is a feature and each row is\n",
    "    an example. You needto perform the normalization separately for each feature. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You might find the 'np.mean' and 'np.std' functions useful.\n",
    "    \"\"\"\n",
    "    mu = []\n",
    "    sigma = []\n",
    "    X_norm = np.copy(X)\n",
    "    \n",
    "    col_to_remove = []\n",
    "    for i in range(X_norm.shape[1]):\n",
    "        mu.append(np.mean(X_norm[:,i]))\n",
    "        sigma.append(np.std(X_norm[:,i]))\n",
    "    X_norm = np.delete(X_norm, col_to_remove, axis=1)\n",
    "\n",
    "    # =========================== YOUR CODE HERE =====================\n",
    "    for i in range(X_norm.shape[1]):\n",
    "        if np.std(X_norm[:,i]) != 0:\n",
    "            X_norm[:,i] = (X_norm[:,i] - np.mean(X_norm[:,i])) / np.std(X_norm[:,i])\n",
    "        else:\n",
    "            X_norm[:,i] = (X_norm[:,i] - np.mean(X_norm[:,i])) / 0.01\n",
    "    \n",
    "    \n",
    "    # ================================================================\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>-0.233570</td>\n",
       "      <td>-0.205885</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>0.992930</td>\n",
       "      <td>0.823953</td>\n",
       "      <td>0.462009</td>\n",
       "      <td>0.571581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235641</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>0.231036</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-1.615345</td>\n",
       "      <td>0.153084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.865696</td>\n",
       "      <td>0.384834</td>\n",
       "      <td>-0.064358</td>\n",
       "      <td>-0.153825</td>\n",
       "      <td>2.284122</td>\n",
       "      <td>0.120665</td>\n",
       "      <td>-0.460746</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>1.152559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224712</td>\n",
       "      <td>1.686090</td>\n",
       "      <td>-0.716739</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-0.498715</td>\n",
       "      <td>-0.596291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>-0.109889</td>\n",
       "      <td>0.138702</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>0.928317</td>\n",
       "      <td>0.776371</td>\n",
       "      <td>0.282510</td>\n",
       "      <td>0.101672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549518</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>-0.064173</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>0.990125</td>\n",
       "      <td>0.153084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331793</td>\n",
       "      <td>-0.439705</td>\n",
       "      <td>-0.070512</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>-1.850006</td>\n",
       "      <td>-0.746235</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>-0.475034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727382</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>-0.172934</td>\n",
       "      <td>4.083851</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-1.615345</td>\n",
       "      <td>-1.345665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>0.549742</td>\n",
       "      <td>0.509132</td>\n",
       "      <td>1.295234</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>0.896011</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>1.275032</td>\n",
       "      <td>0.462647</td>\n",
       "      <td>...</td>\n",
       "      <td>1.742250</td>\n",
       "      <td>0.815913</td>\n",
       "      <td>0.588393</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>2.106755</td>\n",
       "      <td>0.153084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>-0.357251</td>\n",
       "      <td>-0.271480</td>\n",
       "      <td>-0.153825</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>0.863705</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>-0.936399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224712</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>-0.095247</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>0.617915</td>\n",
       "      <td>-0.596291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.865696</td>\n",
       "      <td>0.590968</td>\n",
       "      <td>0.375605</td>\n",
       "      <td>-0.153825</td>\n",
       "      <td>0.411042</td>\n",
       "      <td>0.185277</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>0.055497</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015461</td>\n",
       "      <td>2.104761</td>\n",
       "      <td>-0.716739</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-1.615345</td>\n",
       "      <td>1.651832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331793</td>\n",
       "      <td>-0.192343</td>\n",
       "      <td>-0.133030</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>3.220663</td>\n",
       "      <td>-1.010048</td>\n",
       "      <td>0.966697</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>-0.349013</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.312819</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>0.215498</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>14.947388</td>\n",
       "      <td>-0.498715</td>\n",
       "      <td>1.651832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.865696</td>\n",
       "      <td>-0.109889</td>\n",
       "      <td>-0.049960</td>\n",
       "      <td>-0.878355</td>\n",
       "      <td>0.411042</td>\n",
       "      <td>-0.719293</td>\n",
       "      <td>0.490883</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>-0.831738</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.375594</td>\n",
       "      <td>2.244317</td>\n",
       "      <td>-0.716739</td>\n",
       "      <td>1.471808</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-0.870925</td>\n",
       "      <td>1.651832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.865696</td>\n",
       "      <td>0.178699</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>-0.878355</td>\n",
       "      <td>0.411042</td>\n",
       "      <td>-0.234702</td>\n",
       "      <td>-0.984142</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>0.836438</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.187268</td>\n",
       "      <td>5.281729</td>\n",
       "      <td>0.339797</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-0.126505</td>\n",
       "      <td>0.153084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6   \\\n",
       "0     1.0  0.092295 -0.233570 -0.205885  0.570704 -0.525499  0.992930   \n",
       "1     1.0 -0.865696  0.384834 -0.064358 -0.153825  2.284122  0.120665   \n",
       "2     1.0  0.092295 -0.109889  0.138702  0.570704 -0.525499  0.928317   \n",
       "3     1.0  0.331793 -0.439705 -0.070512  0.570704 -0.525499 -1.850006   \n",
       "4     1.0  0.092295  0.549742  0.509132  1.295234 -0.525499  0.896011   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "1116  1.0  0.092295 -0.357251 -0.271480 -0.153825 -0.525499  0.863705   \n",
       "1117  1.0 -0.865696  0.590968  0.375605 -0.153825  0.411042  0.185277   \n",
       "1118  1.0  0.331793 -0.192343 -0.133030  0.570704  3.220663 -1.010048   \n",
       "1119  1.0 -0.865696 -0.109889 -0.049960 -0.878355  0.411042 -0.719293   \n",
       "1120  1.0 -0.865696  0.178699 -0.022885 -0.878355  0.411042 -0.234702   \n",
       "\n",
       "            7         8         9   ...        27        28        29  \\\n",
       "0     0.823953  0.462009  0.571581  ...  0.235641 -0.760257  0.231036   \n",
       "1    -0.460746 -0.572748  1.152559  ... -0.224712  1.686090 -0.716739   \n",
       "2     0.776371  0.282510  0.101672  ...  0.549518 -0.760257 -0.064173   \n",
       "3    -0.746235 -0.572748 -0.475034  ...  0.727382 -0.760257 -0.172934   \n",
       "4     0.681208  1.275032  0.462647  ...  1.742250  0.815913  0.588393   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1116  0.681208 -0.572748 -0.936399  ... -0.224712 -0.760257 -0.095247   \n",
       "1117  0.110231  0.055497  0.751000  ... -0.015461  2.104761 -0.716739   \n",
       "1118  0.966697 -0.572748 -0.349013  ... -1.312819 -0.760257  0.215498   \n",
       "1119  0.490883 -0.572748 -0.831738  ... -1.375594  2.244317 -0.716739   \n",
       "1120 -0.984142 -0.572748  0.836438  ... -1.187268  5.281729  0.339797   \n",
       "\n",
       "            30       31        32        33         34        35        36  \n",
       "0    -0.356622 -0.11253 -0.278676 -0.072999  -0.141407 -1.615345  0.153084  \n",
       "1    -0.356622 -0.11253 -0.278676 -0.072999  -0.141407 -0.498715 -0.596291  \n",
       "2    -0.356622 -0.11253 -0.278676 -0.072999  -0.141407  0.990125  0.153084  \n",
       "3     4.083851 -0.11253 -0.278676 -0.072999  -0.141407 -1.615345 -1.345665  \n",
       "4    -0.356622 -0.11253 -0.278676 -0.072999  -0.141407  2.106755  0.153084  \n",
       "...        ...      ...       ...       ...        ...       ...       ...  \n",
       "1116 -0.356622 -0.11253 -0.278676 -0.072999  -0.141407  0.617915 -0.596291  \n",
       "1117 -0.356622 -0.11253 -0.278676 -0.072999  -0.141407 -1.615345  1.651832  \n",
       "1118 -0.356622 -0.11253 -0.278676 -0.072999  14.947388 -0.498715  1.651832  \n",
       "1119  1.471808 -0.11253 -0.278676 -0.072999  -0.141407 -0.870925  1.651832  \n",
       "1120 -0.356622 -0.11253 -0.278676 -0.072999  -0.141407 -0.126505  0.153084  \n",
       "\n",
       "[1121 rows x 37 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data, mu, sigma = feature_normalization(data)\n",
    "# replace example # column with 1's intercept column\n",
    "normal_data[:,0] = np.ones(data.shape[0])\n",
    "pd.DataFrame(normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_multiple(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression with multiple variables.\n",
    "    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        A vector of shape (m, ) for the values at a given data point.\n",
    "    \n",
    "    theta : array_like\n",
    "        The linear regression parameters. A vector of shape (n+1, )\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The value of the cost function. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to the cost.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    #number of features\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variable correctly\n",
    "    J = 0\n",
    "        \n",
    "    # ======================= YOUR CODE HERE ==========================\n",
    "    outerSum = 0\n",
    "    for i in range(m):\n",
    "        innerSum = 0\n",
    "        for j in range(n):\n",
    "            innerSum += theta[j] * X[:,j][i]\n",
    "        innerSum -= y[i]\n",
    "        innerSum **= 2\n",
    "        outerSum += innerSum\n",
    "    \n",
    "    J = (1 / (2 * m)) * outerSum\n",
    "            \n",
    "    return J\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CHECKPOINT 7][8 points]\n",
    "def gradient_descent_multiple(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn theta.\n",
    "    Updates theta by taking num_iters gradient steps with learning rate alpha.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        A vector of shape (m, ) for the values at a given data point.\n",
    "    \n",
    "    theta : array_like\n",
    "        The linear regression parameters. A vector of shape (n+1, )\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate for gradient descent. \n",
    "    \n",
    "    num_iters : int\n",
    "        The number of iterations to run gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    J_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Peform a single gradient step on the parameter vector theta.\n",
    "\n",
    "    While debugging, it can be useful to print out the values of \n",
    "    the cost function (computeCost) and gradient here.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Number of training sets and theta values\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # make a copy of theta, which will be updated by gradient descent\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    J_history = []\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        #partialDerivate = 0\n",
    "        # ======================= YOUR CODE HERE ==========================\n",
    "        for j in range(n): #for each theta to be changed\n",
    "            partialDerivativeSum = 0\n",
    "            for i in range(m): # PD sum\n",
    "                hypothesisSum = 0\n",
    "                for k in range(n): # for hypothesis calcs\n",
    "                    #theta 0 * X[:,0][i] + theta1 * X[:,1][i] ...>\n",
    "                    hypothesisSum += theta[k] * X[:,k][i]\n",
    "                partialDerivativeSum += (hypothesisSum - y[i])*X[:,j][i]\n",
    "            partialDerivative = (1 / m) * partialDerivativeSum\n",
    "            theta[j] -= (alpha * partialDerivative)\n",
    "        # =================================================================\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        J_history.append(compute_cost_multiple(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta computed from gradient descent: [ 1.85499650e+05 -8.04198609e+03 -2.56471641e+03  4.37605553e+03\n",
      "  2.54505926e+04  4.97508058e+03  7.52235818e+03  3.48493404e+03\n",
      "  6.17744862e+03  5.00803662e+03  1.72060874e+02 -9.67797367e+02\n",
      "  4.10026720e+03  7.24524990e+03  8.76336537e+03  2.77165924e+02\n",
      "  1.21345263e+04  4.53101107e+03  5.60219449e+02  3.88592893e+03\n",
      "  1.82398194e+02 -7.45837777e+03 -4.68595684e+03  8.80113972e+03\n",
      "  3.16600344e+03 -4.81651635e+02  9.99673459e+03  2.14934778e+03\n",
      "  2.66298757e+03 -2.67755570e+02  2.55735500e+02  1.00750235e+03\n",
      "  3.31355380e+03 -2.51892061e+03 -6.74893027e+02 -5.42365920e+02\n",
      " -3.84460695e+02]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose some alpha value - change this\n",
    "alpha = 0.05\n",
    "num_iters = 200\n",
    "\n",
    "# init theta and run gradient descent\n",
    "theta = np.zeros(normal_data.shape[1])\n",
    "theta, J_history = gradient_descent_multiple(normal_data, y, theta, alpha, num_iters)\n",
    "\n",
    "# Plot the convergence graph\n",
    "plt.plot(np.arange(len(J_history)), J_history, lw=2)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "\n",
    "# Display the gradient descent's result\n",
    "print('theta computed from gradient descent: {:s}'.format(str(theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using theta and gather error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy of prediction:  0.8726193755882231\n",
      "predictions: \n",
      "                  0\n",
      "0     231992.213607\n",
      "1     196460.513633\n",
      "2     225568.125026\n",
      "3     201406.975064\n",
      "4     298982.709000\n",
      "...             ...\n",
      "1116  183371.530622\n",
      "1117  247252.848479\n",
      "1118  219148.602504\n",
      "1119  131929.803272\n",
      "1120  150043.103184\n",
      "\n",
      "[1121 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "accuracy = []\n",
    "for i in range(normal_data.shape[0]):\n",
    "    predict = np.matmul(np.transpose(theta), normal_data[i,:])\n",
    "    prediction.append(predict)\n",
    "    diff = np.abs(y[i] - predict)\n",
    "    accuracy.append(1 - (diff / y[i]))\n",
    "\n",
    "print(\"mean accuracy of prediction: \", np.mean(accuracy))\n",
    "print(\"predictions: \")\n",
    "print(pd.DataFrame(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ae0c22c096cf912027336b166abf37ab4ad484d02de68fa86080dc45c1de523"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

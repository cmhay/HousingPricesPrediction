{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Read CSV file\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
    "\n",
    "# Feature engineering\n",
    "import math\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (1121, 37)\n",
      "Y shape: (1121,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>...</td>\n",
       "      <td>548.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>...</td>\n",
       "      <td>460.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>...</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>642.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2006.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>...</td>\n",
       "      <td>836.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>1456.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>1457.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>...</td>\n",
       "      <td>500.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>1458.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>...</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>1459.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>1460.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>...</td>\n",
       "      <td>276.0</td>\n",
       "      <td>736.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2        3    4    5       6       7      8      9   \\\n",
       "0        1.0  60.0  65.0   8450.0  7.0  5.0  2003.0  2003.0  196.0  706.0   \n",
       "1        2.0  20.0  80.0   9600.0  6.0  8.0  1976.0  1976.0    0.0  978.0   \n",
       "2        3.0  60.0  68.0  11250.0  7.0  5.0  2001.0  2002.0  162.0  486.0   \n",
       "3        4.0  70.0  60.0   9550.0  7.0  5.0  1915.0  1970.0    0.0  216.0   \n",
       "4        5.0  60.0  84.0  14260.0  8.0  5.0  2000.0  2000.0  350.0  655.0   \n",
       "...      ...   ...   ...      ...  ...  ...     ...     ...    ...    ...   \n",
       "1116  1456.0  60.0  62.0   7917.0  6.0  5.0  1999.0  2000.0    0.0    0.0   \n",
       "1117  1457.0  20.0  85.0  13175.0  6.0  6.0  1978.0  1988.0  119.0  790.0   \n",
       "1118  1458.0  70.0  66.0   9042.0  7.0  9.0  1941.0  2006.0    0.0  275.0   \n",
       "1119  1459.0  20.0  68.0   9717.0  5.0  6.0  1950.0  1996.0    0.0   49.0   \n",
       "1120  1460.0  20.0  75.0   9937.0  5.0  6.0  1965.0  1965.0    0.0  830.0   \n",
       "\n",
       "      ...     27     28    29     30   31   32   33      34    35      36  \n",
       "0     ...  548.0    0.0  61.0    0.0  0.0  0.0  0.0     0.0   2.0  2008.0  \n",
       "1     ...  460.0  298.0   0.0    0.0  0.0  0.0  0.0     0.0   5.0  2007.0  \n",
       "2     ...  608.0    0.0  42.0    0.0  0.0  0.0  0.0     0.0   9.0  2008.0  \n",
       "3     ...  642.0    0.0  35.0  272.0  0.0  0.0  0.0     0.0   2.0  2006.0  \n",
       "4     ...  836.0  192.0  84.0    0.0  0.0  0.0  0.0     0.0  12.0  2008.0  \n",
       "...   ...    ...    ...   ...    ...  ...  ...  ...     ...   ...     ...  \n",
       "1116  ...  460.0    0.0  40.0    0.0  0.0  0.0  0.0     0.0   8.0  2007.0  \n",
       "1117  ...  500.0  349.0   0.0    0.0  0.0  0.0  0.0     0.0   2.0  2010.0  \n",
       "1118  ...  252.0    0.0  60.0    0.0  0.0  0.0  0.0  2500.0   5.0  2010.0  \n",
       "1119  ...  240.0  366.0   0.0  112.0  0.0  0.0  0.0     0.0   4.0  2010.0  \n",
       "1120  ...  276.0  736.0  68.0    0.0  0.0  0.0  0.0     0.0   6.0  2008.0  \n",
       "\n",
       "[1121 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import training CSV\n",
    "fname = './data/train.csv'\n",
    "# use pandas to read the CSV properly of all different types\n",
    "data_pd = pd.read_csv(fname)\n",
    "# convert to a numpy array\n",
    "data = np.array(data_pd)\n",
    "# remove categorical data - force cast attempt then remove failures\n",
    "cols_to_remove = []\n",
    "for i in range(data.shape[1]):\n",
    "    try:\n",
    "        data[:,i] = data[:,i].astype(float)\n",
    "    except:\n",
    "        cols_to_remove.append(i)\n",
    "\n",
    "data = np.delete(data, cols_to_remove, axis=1)\n",
    "# remove rows with nan values\n",
    "rows_to_remove = []\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        if math.isnan(data[i,j]):\n",
    "            rows_to_remove.append(i)\n",
    "            break\n",
    "\n",
    "data = np.delete(data, rows_to_remove, axis=0)\n",
    "\n",
    "# remove outliers with standard deviation TODO()\n",
    "\n",
    "# take the actual prices and add them to results y\n",
    "y = data[:, data.shape[1] - 1]\n",
    "# remove prices column from data\n",
    "data = np.delete(data, data.shape[1] - 1, 1)\n",
    "data = data.astype(float)\n",
    "print(\"data shape\", data.shape)\n",
    "print(\"Y shape:\", y.shape)\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization of housing prices\n",
    "fig = plt.figure()\n",
    "fig.add_subplot()\n",
    "sns.histplot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CHECKPOINT 5][5 points]\n",
    "def  feature_normalization(X):\n",
    "    \"\"\"\n",
    "    Normalizes the features in X. returns a normalized version of X where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_norm : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    First, for each feature dimension, compute the mean of the feature\n",
    "    and subtract it from the dataset, storing the mean value in mu. \n",
    "    Next, compute the  standard deviation of each feature and divide\n",
    "    each feature by it's standard deviation, storing the standard deviation \n",
    "    in sigma. \n",
    "    \n",
    "    Note that X is a matrix where each column is a feature and each row is\n",
    "    an example. You needto perform the normalization separately for each feature. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You might find the 'np.mean' and 'np.std' functions useful.\n",
    "    \"\"\"\n",
    "    mu = []\n",
    "    sigma = []\n",
    "    X_norm = np.copy(X)\n",
    "    \n",
    "    col_to_remove = []\n",
    "    for i in range(X_norm.shape[1]):\n",
    "        mu.append(np.mean(X_norm[:,i]))\n",
    "        sigma.append(np.std(X_norm[:,i]))\n",
    "    X_norm = np.delete(X_norm, col_to_remove, axis=1)\n",
    "\n",
    "    # =========================== YOUR CODE HERE =====================\n",
    "    for i in range(X_norm.shape[1]):\n",
    "        if np.std(X_norm[:,i]) != 0:\n",
    "            X_norm[:,i] = (X_norm[:,i] - np.mean(X_norm[:,i])) / np.std(X_norm[:,i])\n",
    "        else:\n",
    "            X_norm[:,i] = (X_norm[:,i] - np.mean(X_norm[:,i])) / 0.01\n",
    "    \n",
    "    \n",
    "    # ================================================================\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>-0.233570</td>\n",
       "      <td>-0.205885</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>0.992930</td>\n",
       "      <td>0.823953</td>\n",
       "      <td>0.462009</td>\n",
       "      <td>0.571581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235641</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>0.231036</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-1.615345</td>\n",
       "      <td>0.153084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.865696</td>\n",
       "      <td>0.384834</td>\n",
       "      <td>-0.064358</td>\n",
       "      <td>-0.153825</td>\n",
       "      <td>2.284122</td>\n",
       "      <td>0.120665</td>\n",
       "      <td>-0.460746</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>1.152559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224712</td>\n",
       "      <td>1.686090</td>\n",
       "      <td>-0.716739</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-0.498715</td>\n",
       "      <td>-0.596291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>-0.109889</td>\n",
       "      <td>0.138702</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>0.928317</td>\n",
       "      <td>0.776371</td>\n",
       "      <td>0.282510</td>\n",
       "      <td>0.101672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549518</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>-0.064173</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>0.990125</td>\n",
       "      <td>0.153084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331793</td>\n",
       "      <td>-0.439705</td>\n",
       "      <td>-0.070512</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>-1.850006</td>\n",
       "      <td>-0.746235</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>-0.475034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727382</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>-0.172934</td>\n",
       "      <td>4.083851</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-1.615345</td>\n",
       "      <td>-1.345665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>0.549742</td>\n",
       "      <td>0.509132</td>\n",
       "      <td>1.295234</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>0.896011</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>1.275032</td>\n",
       "      <td>0.462647</td>\n",
       "      <td>...</td>\n",
       "      <td>1.742250</td>\n",
       "      <td>0.815913</td>\n",
       "      <td>0.588393</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>2.106755</td>\n",
       "      <td>0.153084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>-0.357251</td>\n",
       "      <td>-0.271480</td>\n",
       "      <td>-0.153825</td>\n",
       "      <td>-0.525499</td>\n",
       "      <td>0.863705</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>-0.936399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224712</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>-0.095247</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>0.617915</td>\n",
       "      <td>-0.596291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.865696</td>\n",
       "      <td>0.590968</td>\n",
       "      <td>0.375605</td>\n",
       "      <td>-0.153825</td>\n",
       "      <td>0.411042</td>\n",
       "      <td>0.185277</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>0.055497</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015461</td>\n",
       "      <td>2.104761</td>\n",
       "      <td>-0.716739</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-1.615345</td>\n",
       "      <td>1.651832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331793</td>\n",
       "      <td>-0.192343</td>\n",
       "      <td>-0.133030</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>3.220663</td>\n",
       "      <td>-1.010048</td>\n",
       "      <td>0.966697</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>-0.349013</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.312819</td>\n",
       "      <td>-0.760257</td>\n",
       "      <td>0.215498</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>14.947388</td>\n",
       "      <td>-0.498715</td>\n",
       "      <td>1.651832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.865696</td>\n",
       "      <td>-0.109889</td>\n",
       "      <td>-0.049960</td>\n",
       "      <td>-0.878355</td>\n",
       "      <td>0.411042</td>\n",
       "      <td>-0.719293</td>\n",
       "      <td>0.490883</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>-0.831738</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.375594</td>\n",
       "      <td>2.244317</td>\n",
       "      <td>-0.716739</td>\n",
       "      <td>1.471808</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-0.870925</td>\n",
       "      <td>1.651832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.865696</td>\n",
       "      <td>0.178699</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>-0.878355</td>\n",
       "      <td>0.411042</td>\n",
       "      <td>-0.234702</td>\n",
       "      <td>-0.984142</td>\n",
       "      <td>-0.572748</td>\n",
       "      <td>0.836438</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.187268</td>\n",
       "      <td>5.281729</td>\n",
       "      <td>0.339797</td>\n",
       "      <td>-0.356622</td>\n",
       "      <td>-0.11253</td>\n",
       "      <td>-0.278676</td>\n",
       "      <td>-0.072999</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>-0.126505</td>\n",
       "      <td>0.153084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6   \\\n",
       "0     1.0  0.092295 -0.233570 -0.205885  0.570704 -0.525499  0.992930   \n",
       "1     1.0 -0.865696  0.384834 -0.064358 -0.153825  2.284122  0.120665   \n",
       "2     1.0  0.092295 -0.109889  0.138702  0.570704 -0.525499  0.928317   \n",
       "3     1.0  0.331793 -0.439705 -0.070512  0.570704 -0.525499 -1.850006   \n",
       "4     1.0  0.092295  0.549742  0.509132  1.295234 -0.525499  0.896011   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "1116  1.0  0.092295 -0.357251 -0.271480 -0.153825 -0.525499  0.863705   \n",
       "1117  1.0 -0.865696  0.590968  0.375605 -0.153825  0.411042  0.185277   \n",
       "1118  1.0  0.331793 -0.192343 -0.133030  0.570704  3.220663 -1.010048   \n",
       "1119  1.0 -0.865696 -0.109889 -0.049960 -0.878355  0.411042 -0.719293   \n",
       "1120  1.0 -0.865696  0.178699 -0.022885 -0.878355  0.411042 -0.234702   \n",
       "\n",
       "            7         8         9   ...        27        28        29  \\\n",
       "0     0.823953  0.462009  0.571581  ...  0.235641 -0.760257  0.231036   \n",
       "1    -0.460746 -0.572748  1.152559  ... -0.224712  1.686090 -0.716739   \n",
       "2     0.776371  0.282510  0.101672  ...  0.549518 -0.760257 -0.064173   \n",
       "3    -0.746235 -0.572748 -0.475034  ...  0.727382 -0.760257 -0.172934   \n",
       "4     0.681208  1.275032  0.462647  ...  1.742250  0.815913  0.588393   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1116  0.681208 -0.572748 -0.936399  ... -0.224712 -0.760257 -0.095247   \n",
       "1117  0.110231  0.055497  0.751000  ... -0.015461  2.104761 -0.716739   \n",
       "1118  0.966697 -0.572748 -0.349013  ... -1.312819 -0.760257  0.215498   \n",
       "1119  0.490883 -0.572748 -0.831738  ... -1.375594  2.244317 -0.716739   \n",
       "1120 -0.984142 -0.572748  0.836438  ... -1.187268  5.281729  0.339797   \n",
       "\n",
       "            30       31        32        33         34        35        36  \n",
       "0    -0.356622 -0.11253 -0.278676 -0.072999  -0.141407 -1.615345  0.153084  \n",
       "1    -0.356622 -0.11253 -0.278676 -0.072999  -0.141407 -0.498715 -0.596291  \n",
       "2    -0.356622 -0.11253 -0.278676 -0.072999  -0.141407  0.990125  0.153084  \n",
       "3     4.083851 -0.11253 -0.278676 -0.072999  -0.141407 -1.615345 -1.345665  \n",
       "4    -0.356622 -0.11253 -0.278676 -0.072999  -0.141407  2.106755  0.153084  \n",
       "...        ...      ...       ...       ...        ...       ...       ...  \n",
       "1116 -0.356622 -0.11253 -0.278676 -0.072999  -0.141407  0.617915 -0.596291  \n",
       "1117 -0.356622 -0.11253 -0.278676 -0.072999  -0.141407 -1.615345  1.651832  \n",
       "1118 -0.356622 -0.11253 -0.278676 -0.072999  14.947388 -0.498715  1.651832  \n",
       "1119  1.471808 -0.11253 -0.278676 -0.072999  -0.141407 -0.870925  1.651832  \n",
       "1120 -0.356622 -0.11253 -0.278676 -0.072999  -0.141407 -0.126505  0.153084  \n",
       "\n",
       "[1121 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data, mu, sigma = feature_normalization(data)\n",
    "# replace example # column with 1's intercept column\n",
    "normal_data[:,0] = np.ones(data.shape[0])\n",
    "pd.DataFrame(normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize features with reasonable correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1121, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\colli\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2642: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\colli\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2643: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "top_normal_data = normal_data.copy()\n",
    "corr_threshold = 0.5\n",
    "cols = []\n",
    "for i in range(normal_data.shape[1]):\n",
    "    corr = np.corrcoef(list(normal_data[:,i]), list(y))\n",
    "    if abs(corr[0,1]) < corr_threshold:\n",
    "        cols.append(i)\n",
    "top_normal_data = np.delete(top_normal_data, cols, axis=1)\n",
    "\n",
    "print(top_normal_data.shape)\n",
    "normal_data = top_normal_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_multiple(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression with multiple variables.\n",
    "    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        A vector of shape (m, ) for the values at a given data point.\n",
    "    \n",
    "    theta : array_like\n",
    "        The linear regression parameters. A vector of shape (n+1, )\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The value of the cost function. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to the cost.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    #number of features\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variable correctly\n",
    "    J = 0\n",
    "        \n",
    "    # ======================= YOUR CODE HERE ==========================\n",
    "    outerSum = 0\n",
    "    for i in range(m):\n",
    "        innerSum = 0\n",
    "        for j in range(n):\n",
    "            innerSum += theta[j] * X[:,j][i]\n",
    "        innerSum -= y[i]\n",
    "        innerSum **= 2\n",
    "        outerSum += innerSum\n",
    "    \n",
    "    J = (1 / (2 * m)) * outerSum\n",
    "            \n",
    "    return J\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CHECKPOINT 7][8 points]\n",
    "def gradient_descent_multiple(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn theta.\n",
    "    Updates theta by taking num_iters gradient steps with learning rate alpha.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        A vector of shape (m, ) for the values at a given data point.\n",
    "    \n",
    "    theta : array_like\n",
    "        The linear regression parameters. A vector of shape (n+1, )\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate for gradient descent. \n",
    "    \n",
    "    num_iters : int\n",
    "        The number of iterations to run gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    J_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Peform a single gradient step on the parameter vector theta.\n",
    "\n",
    "    While debugging, it can be useful to print out the values of \n",
    "    the cost function (computeCost) and gradient here.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Number of training sets and theta values\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # make a copy of theta, which will be updated by gradient descent\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    J_history = []\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        #partialDerivate = 0\n",
    "        # ======================= YOUR CODE HERE ==========================\n",
    "        for j in range(n): #for each theta to be changed\n",
    "            partialDerivativeSum = 0\n",
    "            for i in range(m): # PD sum\n",
    "                hypothesisSum = 0\n",
    "                for k in range(n): # for hypothesis calcs\n",
    "                    #theta 0 * X[:,0][i] + theta1 * X[:,1][i] ...>\n",
    "                    hypothesisSum += theta[k] * X[:,k][i]\n",
    "                partialDerivativeSum += (hypothesisSum - y[i])*X[:,j][i]\n",
    "            partialDerivative = (1 / m) * partialDerivativeSum\n",
    "            theta[j] -= (alpha * partialDerivative)\n",
    "        # =================================================================\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        J_history.append(compute_cost_multiple(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta computed from gradient descent: [185499.65005712  30524.07790632   8572.45442661   7302.1731271\n",
      "   9118.77071686   4190.77222864  22489.91686364  -3380.52876983\n",
      "   3645.91087608  -3584.9536118    8850.63287023   4486.65892716]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAERCAYAAABhKjCtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkN0lEQVR4nO3de3Rc5X3u8e8zkizf5BsSxvEFmzuOiw0xTgkJl1wI5CR104YEH05DGlqXFNJDu5oWTtYJXck5TVOarrYJCXE4lNLFpQmBhIZ7EggpCWCZi7HBJo6xsfBN2Mb3m6Tf+WPvkcfDjCzZ2pqx5vmsNUsz79575uctWY/e/e79bkUEZmZmxXKVLsDMzKqTA8LMzEpyQJiZWUkOCDMzK8kBYWZmJTkgzMyspEEXEJJulbRR0pJerHuepOckdUj6RNGyKyT9On1ckV3FZmbVadAFBHAbcHEv130d+AxwZ2GjpHHADcC7gTnADZLG9l+JZmbVb9AFREQ8CWwubJN0oqSHJS2S9AtJp6XrroqIxUBX0dt8GHgsIjZHxBbgMXofOmZmg0J9pQsYIAuAqyLi15LeDXwLeH8P608E1hS8bkvbzMxqxqAPCEkjgfcA35eUb2481GYl2jwniZnVlEEfECSH0d6KiFl92KYNuKDg9STgif4rycys+g26MYhiEbENeE3SpQBKzDzEZo8AF0kamw5OX5S2mZnVjEEXEJLuAn4FnCqpTdKVwOXAlZJeBJYCc9N1z5bUBlwKfEfSUoCI2Ax8BViYPr6ctpmZ1Qx5um8zMytl0PUgzMysfwyqQerm5uaYOnVqpcswMztqLFq06M2IaCm1bFAFxNSpU2ltba10GWZmRw1Jq8st8yEmMzMryQFhZmYlOSDMzKwkB4SZmZXkgDAzs5IcEGZmVpIDwszMSqr5gFi2fhvzFjzN9fcurnQpZmZVZVBdKHc4OjqDX63cxNbd+ytdiplZVcksICTdCnwU2BgRM0os/wLJLKv5Ok4HWiJis6RVwHagE+iIiNlZ1TlqaAMA2/Y4IMzMCmV5iOk2eriPc0TcGBGz0hv5XA/8vGhK7QvT5ZmFA8CoYUlGbnMPwszsIJkFREQ8CfT2HgrzgLuyqqUnIxuTgNi+t4OuLk99bmaWV/FBaknDSXoaPyhoDuBRSYskzc/y8+vrcoxsrCcCduzryPKjzMyOKhUPCOBjwFNFh5fOjYizgEuAqyWdV25jSfMltUpqbW9vP6wCRg31YSYzs2LVEBCXUXR4KSLWpl83AvcBc8ptHBELImJ2RMxuaSk5pfkhjRqWDlTvdg/CzCyvogEhaTRwPvCjgrYRkpryz4GLgCVZ1uEzmczM3i7L01zvAi4AmiW1ATcADQARcXO62seBRyNiZ8Gm44H7JOXruzMiHs6qToAmH2IyM3ubzAIiIub1Yp3bSE6HLWxbCczMpqrS8oeYtu/xISYzs7xqGIOouO5Bah9iMjPr5oDAg9RmZqU4IPAgtZlZKQ4IPN2GmVkpDgjcgzAzK8UBgccgzMxKcUBQcB2EexBmZt0cEPgQk5lZKQ4IfKGcmVkpDggOnmojwveEMDMDBwQADXU5hg+poytg577OSpdjZlYVHBCp7nEIXwthZgY4ILp1XyzngWozM8AB0e1AD8ID1WZm4IDolj+TaasPMZmZAQ6IbmPSgHhr174KV2JmVh0cEKkxw4cA8NYu9yDMzMAB0W3ciKQHsdk9CDMzwAHRbeyIpAexZacDwswMHBDdxqWHmDY7IMzMgAwDQtKtkjZKWlJm+QWStkp6IX18qWDZxZKWS1oh6bqsaiyU70F4DMLMLJFlD+I24OJDrPOLiJiVPr4MIKkOuAm4BJgOzJM0PcM6ARib70F4DMLMDMgwICLiSWDzYWw6B1gRESsjYh9wNzC3X4srYWw6SO0xCDOzRKXHIM6R9KKkhyS9M22bCKwpWKctbctUvgexZdc+uro8o6uZWSUD4jng+IiYCXwD+GHarhLrlv2NLWm+pFZJre3t7YddTENdjqah9XSF7wthZgYVDIiI2BYRO9LnDwINkppJegyTC1adBKzt4X0WRMTsiJjd0tJyRDV5HMLM7ICKBYSk4yQpfT4nrWUTsBA4WdI0SUOAy4D7B6Km/JlMPtXVzAzqs3pjSXcBFwDNktqAG4AGgIi4GfgE8DlJHcBu4LJIbufWIeka4BGgDrg1IpZmVWehccM9UG1mlpdZQETEvEMs/ybwzTLLHgQezKKunnRfTe1DTGZmFT+LqaqMG+6AMDPLc0AUODAG4aupzcwcEAW6r4XwGISZmQOikKf8NjM7wAFRYGz3TYMcEGZmDogC49IxiE0+xGRm5oAodMzIRgA27XBAmJk5IAqMGdZAfU5s3b2fvR2dlS7HzKyiHBAFcjnRnPYi3nQvwsxqnAOiSHNTMg7Rvn1vhSsxM6ssB0SRlnwPwgFhZjXOAVGkpSkJiPYdDggzq20OiCL5MQgfYjKzWueAKJLvQbzpHoSZ1TgHRJHuQ0zuQZhZjXNAFPEhJjOzhAOiiA8xmZklHBBFfIjJzCzhgCjS1FjPkPocO/d1smtfR6XLMTOrGAdEEUkFF8t5ug0zq12ZBYSkWyVtlLSkzPLLJS1OH7+UNLNg2SpJL0l6QVJrVjWWc+BiuT0D/dFmZlUjyx7EbcDFPSx/DTg/Is4AvgIsKFp+YUTMiojZGdVXlschzMygPqs3jognJU3tYfkvC14+DUzKqpa+ygfEhm0OCDOrXdUyBnEl8FDB6wAelbRI0vyeNpQ0X1KrpNb29vZ+KWbCqKEArN/mQ0xmVrsy60H0lqQLSQLivQXN50bEWknHAo9JWhYRT5baPiIWkB6emj17dvRHTeNHJwGxYasDwsxqV0V7EJLOAG4B5kbEpnx7RKxNv24E7gPmDGRdE9KAWOeAMLMaVrGAkDQFuBf4g4h4taB9hKSm/HPgIqDkmVBZOc6HmMzMsjvEJOku4AKgWVIbcAPQABARNwNfAo4BviUJoCM9Y2k8cF/aVg/cGREPZ1VnKcelPYj1W/cQEaS1mJnVlCzPYpp3iOV/BPxRifaVwMy3bzFwmoY2MLKxnh17O9i2u4PRwxsqWY6ZWUVUy1lMVWf8qORU13Xbdle4EjOzynBAlDFh9DAgOcxkZlaLHBBlFI5DmJnVIgdEGfkzmXyqq5nVKgdEGfkexAaf6mpmNcoBUYZ7EGZW6xwQZXgMwsxqnQOijPx0G2u3+jRXM6tNDogyxo0YwtCGHNv3dLB19/5Kl2NmNuAcEGVIYtLY4QC8scW9CDOrPQ6IHkwck1ws98ZbDggzqz0OiB5MGpsERNuWXRWuxMxs4DkgeuBDTGZWyxwQPZjY3YNwQJhZ7XFA9KD7ENNbPsRkZrXHAdGDfED4EJOZ1SIHRA+aRzQypD7Hll372bm3o9LlmJkNKAdED3I5McmnuppZjXJAHMJEn+pqZjWq7D2pJY3rYbu9EbEzg3qqTn4cYs1m9yDMrLb01INYBLSmX4sfyyStkXR5uY0l3Sppo6QlZZZL0r9IWiFpsaSzCpZdLGl5uuy6w/mH9ZfJ45JrIV7f7B6EmdWWsj2IiJjW04aSWoCfA3eUWeU24JvA7WWWXwKcnD7eDXwbeLekOuAm4ENAG7BQ0v0R8XJP9WRl6jEjAFi9qSY6TGZm3Q57DCIi2oG/7mH5k8DmHt5iLnB7JJ4GxkiaAMwBVkTEyojYB9ydrlsRxx+T9CBWb3IPwsxqyxENUkfEfx7B5hOBNQWv29K2cu0lSZovqVVSa3t7+xGUU9rx+R7E5l10dUW/v7+ZWbWq5FlMKtEWPbSXFBELImJ2RMxuaWnpt+LyRjbW0zxyCPs6uljv+1ObWQ05ZEBI+vfetB2GNmBywetJwNoe2ism34tY5XEIM6shvelBvLPwRTqI/K5++Oz7gU+nZzP9NrA1ItYBC4GTJU2TNAS4LF23YvLjEK97HMLMakhP10FcD/wvYJikbflmYB+w4FBvLOku4AKgWVIbcAPQABARNwMPAh8BVgC7gD9Ml3VIugZ4BKgDbo2IpYfzj+svx4/L9yAcEGZWO3o6zfWrwFclfTUiru/rG0fEvEMsD+DqMsseJAmQqjC1OX8mkw8xmVnt6M0hph9LGgEg6X9I+kdJx2dcV1U5MAbhHoSZ1Y7eBMS3gV2SZgJ/Baym/MVvg9LUYw70IJKOj5nZ4NebgOhIDwfNBf45Iv4ZaMq2rOoyZvgQxg5vYNe+TjZs21vpcszMBkRvAmJ7OmD9B8AD6VlMDdmWVX1ObBkJwG/ad1S4EjOzgdGbgPgUsBf4bESsJ7mq+cZMq6pCDggzqzWHDIg0FO4ARkv6KLAnImpqDALgxGOTgerfbHRAmFlt6M2V1J8EngUuBT4JPCPpE1kXVm1OOjbfg/CprmZWG8peB1Hgi8DZEbERuqf5/glwT5aFVRsfYjKzWtObMYhcPhxSm3q53aAyaexwhtTlWLd1Dzv2dlS6HDOzzPXmF/3Dkh6R9BlJnwEeAB7KtqzqU5cT05qTcYjXfJjJzGpAbwapvwB8BzgDmAksiIi/yrqwatQ9UO3DTGZWA3qarO8kYHxEPBUR9wL3pu3nSToxIn4zUEVWi/w4xK83bq9wJWZm2eupB/FPQKnfhLvSZTXnlPHJBeTL17sHYWaDX08BMTUiFhc3RkQrMDWziqrYacelAbFh2yHWNDM7+vUUEEN7WDasvws5GkxtHsGQuhxrNu/2mUxmNuj1FBALJf1xcaOkK4FF2ZVUvRrqcpzQkgxUv7rB4xBmNrj1dKHctcB9ki7nQCDMBoYAH8+4rqp12nFNLFu/nVfXb+esKWMrXY6ZWWZ6uqPcBuA9ki4EZqTND0TEzwaksip16nGjgLUsW+8ehJkNboecaiMiHgceH4BajgrdA9UOCDMb5GpuyowjdWr3mUzbfXc5MxvUMg0ISRdLWi5phaTrSiz/gqQX0scSSZ2SxqXLVkl6KV3WmmWdfTFh9FBGDa1n8859vrucmQ1qmQVEeue5m4BLgOnAPEnTC9eJiBsjYlZEzAKuB34eEZsLVrkwXT47qzr7ShLvfMdoAJau3VrhaszMspNlD2IOsCIiVkbEPuBukvtalzMPuCvDevrNjImjAFjyhi+YM7PBK8uAmAisKXjdlra9jaThwMXADwqaA3hU0iJJ88t9iKT5kloltba3t/dD2Yc2Y2LSg1jiHoSZDWJZBoRKtJUb1f0Y8FTR4aVzI+IskkNUV0s6r9SGEbEgImZHxOyWlpYjq7iX8oeYXl7rHoSZDV5ZBkQbMLng9SRgbZl1L6Po8FJErE2/bgTuIzlkVRWmNY9gWEMdb7y1m80791W6HDOzTGQZEAuBkyVNkzSEJATuL15J0mjgfOBHBW0jJDXlnwMXAUsyrLVP6nJi+juScQgPVJvZYJVZQEREB3AN8AjwCvC9iFgq6SpJVxWs+nHg0YgovE3beOC/JL0IPEtyBffDWdV6OGakAfHSGw4IMxucDnkl9ZGIiAeBB4vabi56fRtwW1HbSpK711Wt/ED14jUOCDMbnHwl9WE6c8oYAF5Y81ZF6zAzy4oD4jCd0DySpqH1rN+2h3Vbd1e6HDOzfueAOEy5nJg1eQwAL7z+VkVrMTPLggPiCJyZDwgfZjKzQcgBcQRmpeMQz7sHYWaDkAPiCMyanNxRbvEbb9HR2VXhaszM+pcD4giMGzGEac0j2LO/i5fXedoNMxtcHBBH6OypSS/i2dc2H2JNM7OjiwPiCJ09dRzggDCzwccBcYTePe0YABau2uxbkJrZoOKAOEKTxw1j/KhGtuzaz4qNOypdjplZv3FAHCFJzEl7Ec+u8mEmMxs8HBD9YM60ZBzi6ZUOCDMbPBwQ/eA9JyY9iF+ueJOuLo9DmNng4IDoByc0j2DC6KFs2rmPZeu3V7ocM7N+4YDoB5I496RmAJ5a8WaFqzEz6x8OiH7yvpOTgPgvB4SZDRIOiH7ynhOTgHjmtU3s7eiscDVmZkfOAdFPWpoaOe24Jvbs72Lha1sqXY6Z2RFzQPSjC087FoCfLttQ4UrMzI5cpgEh6WJJyyWtkHRdieUXSNoq6YX08aXebluNPpAGxM+WbfS0G2Z21KvP6o0l1QE3AR8C2oCFku6PiJeLVv1FRHz0MLetKmdOGcuY4Q2s3rSLlW/u5MSWkZUuyczssGXZg5gDrIiIlRGxD7gbmDsA21ZMXU5ceGrai3hlY4WrMTM7MlkGxERgTcHrtrSt2DmSXpT0kKR39nFbJM2X1Cqptb29vT/qPiLvTw8zPfayxyHM7OiWZUCoRFvxgfnngOMjYibwDeCHfdg2aYxYEBGzI2J2S0vL4dbaby44tYUhdTkWrt7Mxu17Kl2OmdlhyzIg2oDJBa8nAWsLV4iIbRGxI33+INAgqbk321arpqENnHdKMxHwyFL3Iszs6JVlQCwETpY0TdIQ4DLg/sIVJB0nSenzOWk9m3qzbTW7ZMYEAB56aV2FKzEzO3yZncUUER2SrgEeAeqAWyNiqaSr0uU3A58APiepA9gNXBbJ+aElt82q1v72wdPH01Annl65iU079nLMyMZKl2Rm1meZBQR0HzZ6sKjt5oLn3wS+2dttjxajhzfw3pOaeXx5Ow+8tI5PnzO10iWZmfWZr6TOyO+emZx0de9zb1S4EjOzw+OAyMhF049jxJA6XljzFivbfa9qMzv6OCAyMmxIHReng9U/fN69CDM7+jggMvT7ZyWHme5Z1Eanb0VqZkcZB0SGfvuEY5gybjhrt+7h56966g0zO7o4IDKUy4l5c6YAcOczr1e4GjOzvnFAZOzS2ZNoqBM/W7aRtW/trnQ5Zma95oDIWPPIRi6eMYGugNt/tbrS5ZiZ9ZoDYgB89typANz5zGp27u2obDFmZr3kgBgAZ04Zy7uOH8u2PR3cs6it0uWYmfWKA2KA/NF7pwHw3V+sZH9nV4WrMTM7NAfEALnoncdxQssI2rbs5j5fOGdmRwEHxACpy4nPv/8kAG56fAUd7kWYWZVzQAygj53xDqYeM5zVm3Z5Ej8zq3oOiAFUX5fjzz90CgD/+Nir7N7XWeGKzMzKc0AMsI+d8Q5mTBzF+m17uPWp1ypdjplZWQ6IAZbLiesvOR2Abz2+gvVb91S4IjOz0hwQFXDuSc18aPp4du7r5P888HKlyzEzK8kBUSE3fGw6Qxty/HjxOp58tb3S5ZiZvY0DokImjR3On33gZACu+8Fitu/ZX+GKzMwOlmlASLpY0nJJKyRdV2L55ZIWp49fSppZsGyVpJckvSCpNcs6K2X++07gjEmjWbt1D//3gVcqXY6Z2UEyCwhJdcBNwCXAdGCepOlFq70GnB8RZwBfARYULb8wImZFxOys6qyk+roc/3DpTIbU5bh74RoeWLyu0iWZmXXLsgcxB1gRESsjYh9wNzC3cIWI+GVEbElfPg1MyrCeqnTK+Ca++N+Ss5qu+8FiVm/aWeGKzMwSWQbERGBNweu2tK2cK4GHCl4H8KikRZLml9tI0nxJrZJa29uPzsHeT59zPB9+53i27+1g/u2L2OEpwc2sCmQZECrRFiVXlC4kCYi/Lmg+NyLOIjlEdbWk80ptGxELImJ2RMxuaWk50porQhI3XjqTE1tGsHzDdq69+3nP1WRmFZdlQLQBkwteTwLWFq8k6QzgFmBuRGzKt0fE2vTrRuA+kkNWg9aooQ3ccsXZjB7WwE9e2cgX7llMV1fJPDUzGxBZBsRC4GRJ0yQNAS4D7i9cQdIU4F7gDyLi1YL2EZKa8s+Bi4AlGdZaFaY1j+Bf//BsRgyp477n3+CLP1xChEPCzCojs4CIiA7gGuAR4BXgexGxVNJVkq5KV/sScAzwraLTWccD/yXpReBZ4IGIeDirWqvJWVPGcssVZ9NYn+OuZ1/nyz9+2T0JM6sIDaa/UGfPnh2trYPjkoknlm/kj29vZX9n8NEzJvAPl85kaENdpcsys0FG0qJylxL4SuoqdcGpx3LLFWczsrGeHy9ex7zvPs2bO/ZWuiwzqyEOiCp2/ikt3PO5c5g4ZhjPv/4Wc7/5FM++trnSZZlZjXBAVLnTjhvFfVe/h5mTx/DGW7v51IJf8bWHl7Gvw6fBmlm2HBBHgWObhvL9PzmHqy88EQHffuI3zL3JvQkzy5YD4igxpD7HFz58Gt/7k3OYPG4Yr6zbxie/8yuuvuM51mzeVenyzGwQckAcZWZPHccj157HtR88maENOR54aR3v//oT/NU9L7Ji445Kl2dmg4hPcz2Krdu6m79/eDk/euENugIk+ODp4/nvc6bwvpObqa9z/ptZz3o6zdUBMQisenMnC36xknsWtXUPXh/b1MjHz5rIR2ZM4LcmjiaXKzU1lpnVOgdEjdi4fQ/fb23jnkVtvPbmgWnDjxs1lA+cfizvObGZs6eN5dimoRWs0syqiQOixkQErau38J8vruXRpRtYv23PQctPaB7BnGnjOGvKWE6b0MQp45t8lbZZjXJA1LCI4KU3tvLE8nYWrtrMotVb2LWv86B1coKpzSM4/bhRTG0ezpRxw5k8Lvk6YfQw6nx4ymzQ6ikg6ge6GBtYkjhj0hjOmDQGgP2dXSxdu41nX9vEkje28cq6bax8cycr25NHsZzgmJGNHNvUSEtTIy0jGzl2VCPHjGhk9LAGmobWM2pYA6OGNjBqWD1NQxtoaqz3mIfZIOCAqDENdTlmTR7DrMljutv27O9kxcYdLF+/ndc372LN5l28nj42bt9Le/roLQmGNdQxrKGOoQ11NDbkup8nX3M0NtTRWJ+jIZejrk405ER9XY76OlGfE/W5HA11oi79Wp9fnhN1OSGJnCAnofTrgeekyw+sQ/c6B28jIJdT9zY66N+RvFLBvyt5rYNeF//by61TbtnBn9n9rOQ6KnjDw6mrt9THDfr650Bf60k+o4819fXf3LfV+7xB1vWPGdbQ72cuOiCMoQ11zJg4mhkTR79t2b6OLjbt3MvGbUlI5ANj0869bN/Twbbd+5Ove/Z3P9++t4Nd+zrfdijLzLLzk784n5OOHdmv7+mAsB4Nqc8xYfQwJowe1uttOruC3fs72XPQo6ugLXm+r6OLjs4u9ncFnZ1ddHQF+zvjQFtXFx2daVtXsryjs4vOrmRsJYCuCLoi+RoRdHVBkLRFwbL86+h+/fa2zsLhuHRsLg5+SaQthUN3B5blX799XK/c9gd/5MGfSQ/v25e6+qqv45J9/ZjDqSv6+Cl9/Yzs/w3Z1g9kMlbogLB+V5cTIxvrGdnoHy+zo5kvtTUzs5IcEGZmVpIDwszMSnJAmJlZSZkGhKSLJS2XtELSdSWWS9K/pMsXSzqrt9uamVm2MgsISXXATcAlwHRgnqTpRatdApycPuYD3+7DtmZmlqEsexBzgBURsTIi9gF3A3OL1pkL3B6Jp4Exkib0clszM8tQlgExEVhT8LotbevNOr3ZFgBJ8yW1Smptb28/4qLNzCyR5ZVMpS7rK74+sNw6vdk2aYxYACwAkNQuaXVfiizQDLx5mNtmyXX1XbXW5rr6xnX13eHUdny5BVkGRBswueD1JGBtL9cZ0ott3yYiWg6rUkBSa7kpbyvJdfVdtdbmuvrGdfVdf9eW5SGmhcDJkqZJGgJcBtxftM79wKfTs5l+G9gaEet6ua2ZmWUosx5ERHRIugZ4BKgDbo2IpZKuSpffDDwIfARYAewC/rCnbbOq1czM3i7T2dQi4kGSEChsu7ngeQBX93bbjC0YwM/qC9fVd9Vam+vqG9fVd/1a26C65aiZmfUfT7VhZmYlOSDMzKykmg+IapnzSdJkSY9LekXSUkn/M23/G0lvSHohfXykQvWtkvRSWkNr2jZO0mOSfp1+HTvANZ1asF9ekLRN0rWV2GeSbpW0UdKSgray+0fS9enP3HJJH65AbTdKWpbOgXafpDFp+1RJuwv23c1l3zibusp+7wZqn5Wp6z8Kalol6YW0fSD3V7nfEdn9nEV6q8ZafJCcIfUb4ASSay9eBKZXqJYJwFnp8ybgVZJ5qP4G+Msq2FergOaitr8HrkufXwd8rcLfy/UkF/0M+D4DzgPOApYcav+k39cXgUZgWvozWDfAtV0E1KfPv1ZQ29TC9Sqwz0p+7wZyn5Wqq2j514EvVWB/lfsdkdnPWa33IKpmzqeIWBcRz6XPtwOvUGZ6kSoyF/i39Pm/Ab9buVL4APCbiDjcK+mPSEQ8CWwuai63f+YCd0fE3oh4jeQ07zkDWVtEPBoRHenLp0kuRh1QZfZZOQO2z3qqS5KATwJ3ZfHZPenhd0RmP2e1HhC9nvNpIEmaCpwJPJM2XZMeCrh1oA/jFAjgUUmLJM1P28ZHcmEj6ddjK1QbJBdTFv6nrYZ9Vm7/VNvP3WeBhwpeT5P0vKSfS3pfBeop9b2rln32PmBDRPy6oG3A91fR74jMfs5qPSB6PefTQJE0EvgBcG1EbCOZAv1EYBawjqR7WwnnRsRZJFOwXy3pvArV8TZKrrb/HeD7aVO17LNyqubnTtIXgQ7gjrRpHTAlIs4E/gK4U9KoASyp3PeuWvbZPA7+Q2TA91eJ3xFlVy3R1qd9VusB0Zv5ogaMpAaSb/wdEXEvQERsiIjOiOgCvkuGhyJ6EhFr068bgfvSOjYomZ6d9OvGStRGElrPRcSGtMaq2GeU3z9V8XMn6Qrgo8DlkR60Tg9HbEqfLyI5bn3KQNXUw/eu4vtMUj3we8B/5NsGen+V+h1Bhj9ntR4QVTPnU3ps8/8Br0TEPxa0TyhY7ePAkuJtB6C2EZKa8s9JBjiXkOyrK9LVrgB+NNC1pQ76q64a9lmq3P65H7hMUqOkaSQ3zHp2IAuTdDHw18DvRMSugvYWJTfsQtIJaW0rB7Cuct+7iu8z4IPAsohoyzcM5P4q9zuCLH/OBmL0vZofJHNBvUqS/F+sYB3vJen+LQZeSB8fAf4deCltvx+YUIHaTiA5G+JFYGl+PwHHAD8Ffp1+HVeB2oYDm4DRBW0Dvs9IAmodsJ/kL7cre9o/wBfTn7nlwCUVqG0FyfHp/M/azem6v59+j18EngM+NsB1lf3eDdQ+K1VX2n4bcFXRugO5v8r9jsjs58xTbZiZWUm1fojJzMzKcECYmVlJDggzMyvJAWFmZiU5IMzMrCQHhFUtSSHp6wWv/1LS3/TTe98m6RP98V6H+JxL09k3Hy9qf4eke9Lns9SPM85KGiPpT0t9lllfOCCsmu0Ffk9Sc6ULKZS/MKqXrgT+NCIuLGyMiLURkQ+oWSTns/elhp5uFzwG6A6Ios8y6zUHhFWzDpJ77P558YLiHoCkHenXC9JJ074n6VVJfyfpcknPKrmfxYkFb/NBSb9I1/toun2dknslLEwnjPuTgvd9XNKdJBdyFdczL33/JZK+lrZ9ieTippsl3Vi0/tR03SHAl4FPKbmfwKfSK9dvTWt4XtLcdJvPSPq+pP8kmThxpKSfSnou/ez8TMR/B5yYvt+N+c9K32OopH9N139e0oUF732vpIeV3Ffg7wv2x21prS9Jetv3wgavnv4KMasGNwGL87+wemkmcDrJlM0rgVsiYo6SG6x8Hrg2XW8qcD7J5HCPSzoJ+DSwNSLOltQIPCXp0XT9OcCMSKZO7ibpHST3VHgXsIXkl/fvRsSXJb2f5P4GraUKjYh9aZDMjohr0vf7W+BnEfFZJTfyeVbST9JNzgHOiIjNaS/i4xGxLe1lPS3pfpJ7AsyIiFnp+00t+Mir08/9LUmnpbXm5w6aRTJD6F5guaRvkMwMOjEiZqTvNab8brfBxj0Iq2qRzFZ5O/BnfdhsYSRz5+8lmWYg/wv+JZJQyPteRHRFMnXzSuA0knmmPq3kjmHPkExjcHK6/rPF4ZA6G3giItojucfCHSQ3nTlcFwHXpTU8AQwFpqTLHouI/L0KBPytpMXAT0imch5/iPd+L8l0FkTEMmA1ByaX+2lEbI2IPcDLJDdfWgmcIOkb6fxNPc0eaoOMexB2NPgnknlu/rWgrYP0D5x0ErMhBcv2FjzvKnjdxcE/88XzzATJL93PR8QjhQskXQDsLFNfqWmVj4SA34+I5UU1vLuohsuBFuBdEbFf0iqSMDnUe5dTuN86Se44t0XSTODDJL2PT5LcP8JqgHsQVvXSv5i/RzLgm7eK5JAOJHfOajiMt75UUi4dlziBZEKzR4DPKZlWGUmnKJnBtifPAOdLak4HsOcBP+9DHdtJbiGZ9wjw+TT4kHRmme1GAxvTcLiQ5C/+Uu9X6EmSYCE9tDSF5N9dUnroKhcRPwD+N8mtOK1GOCDsaPF1oPBspu+S/FJ+Fij+y7q3lpP8In+IZJbOPcAtJIdXnksHdr/DIXrakdzF63rgcdJZPSOiL1OfPw5Mzw9SA18hCbzFaQ1fKbPdHcBsSa0kv/SXpfVsIhk7WVI8OA58C6iT9BLJfQ0+kx6KK2ci8ER6uOu29N9pNcKzuZqZWUnuQZiZWUkOCDMzK8kBYWZmJTkgzMysJAeEmZmV5IAwM7OSHBBmZlbS/wchKPqSdEnhNAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose some alpha value - change this\n",
    "alpha = 0.05\n",
    "num_iters = 200\n",
    "\n",
    "# init theta and run gradient descent\n",
    "theta = np.zeros(normal_data.shape[1])\n",
    "theta, J_history = gradient_descent_multiple(normal_data, y, theta, alpha, num_iters)\n",
    "\n",
    "# Plot the convergence graph\n",
    "plt.plot(np.arange(len(J_history)), J_history, lw=2)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "\n",
    "# Display the gradient descent's result\n",
    "print('theta computed from gradient descent: {:s}'.format(str(theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using theta and gather error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE helper func\n",
    "def MSE_func(m, hypothesis, actual):\n",
    "    MSE = 0\n",
    "    for i in range(m):\n",
    "            MSE += (hypothesis[i] - actual[i]) ** 2\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 1799602853824.3538\n",
      "predictions: \n",
      "                  0\n",
      "0     217114.790308\n",
      "1     168712.798949\n",
      "2     218584.872648\n",
      "3     200960.608489\n",
      "4     290377.328253\n",
      "...             ...\n",
      "1116  189379.060253\n",
      "1117  225767.883672\n",
      "1118  228139.702869\n",
      "1119  121446.929248\n",
      "1120  129107.526267\n",
      "\n",
      "[1121 rows x 1 columns]\n",
      "Mean Square Error Score: 1605354909.745189\n",
      "VarScore: 0.7667555839737155\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "prediction = []\n",
    "accuracy = []\n",
    "for i in range(normal_data.shape[0]):\n",
    "    predict = np.matmul(np.transpose(theta), normal_data[i,:])\n",
    "    prediction.append(predict)\n",
    "    diff = np.abs(y[i] - predict)\n",
    "    accuracy.append(1 - (diff / y[i]))\n",
    "\n",
    "#print(\"mean accuracy of prediction 1 - (difference / actual): \", np.mean(accuracy))\n",
    "print(\"mean squared error:\", MSE_func(normal_data.shape[0], prediction, y))\n",
    "print(\"predictions: \")\n",
    "print(pd.DataFrame(prediction))\n",
    "print(\"Mean Square Error Score:\", metrics.mean_squared_error(y, prediction))\n",
    "print('VarScore:',metrics.explained_variance_score(y,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d883b1d72725be198cdd764ca4b6ca3a0d1d61c11b349ddfe1157190f28e9012"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
